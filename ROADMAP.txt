- if the ancient dna motifs are observed at the end of the sequences, then cutting them short would render them modern?
- likewise, a smaller kernel migth impact the prediction power significantly?
- given that the architecture deepness is linked to the sequence length, consider using different lengths in training, or training different models
- potentially introduce regularization penalities
- add validation set
- models are mediocre, test without batchnorm
- the model length is a big concern, think about alternatives to mitigate


- techical:
  - add verbose option on cnn training
  - add logging on cnn traing

- todo:
  - genome reference downloaded (refseq h38)
  - generate in two tiers, random sequences => 75nt;
    10k, 20k, 30k, 40k, 50k
    100k, 200k, 300k, 400k, 500k
  - use modern & degragate for ancient

==================================================
src/cnn.jl
  line 3      TODO   load both sequences, french & neandertal, and tag for training
==================================================
