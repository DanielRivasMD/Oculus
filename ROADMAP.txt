- if the ancient dna motifs are observed at the end of the sequences, then cutting them short would render them modern?
- likewise, a smaller kernel migth impact the prediction power significantly?
- given that the architecture deepness is linked to the sequence length, consider using different lengths in training, or training different models
- potentially introduce regularization penalities
- add validation set
- models are mediocre, test without batchnorm
- the model length is a big concern, think about alternatives to mitigate


- techical:
  - add verbose option on cnn training
  - add logging on cnn traing

- todo:
  - genome reference downloaded (refseq h38)
  - generate in two tiers, random sequences => 75nt;
    10k, 20k, 30k, 40k, 50k
    100k, 200k, 300k, 400k, 500k
  - use modern & degragate for ancient

simulation plan:
set up gargammel (cli tool to simulate ancient sequences)
importantly
  - gargammel was cloned & patch for python version compatibility
  - gargammel only works on bash through conda enviroment
  - gargammel required to install several python packages, conda, and ms (samples under neutral models - rhudson, uchicago)
  - output writes to current directory, thus preferably run from gdata (gargammel data)
  - downloaded a reference human genome (h38) & softlink to gdata
  - manipulate gargammel params to create 100% bacterial contamination sample, where bacteria is in fact human sequences
  - declare softlinked human reference genome at gdata/bact/list
  - use gargammel params: composition = 0,0,1; mapdamage = missincorporation.txt (from gargammel Ust_Ishim)

==================================================
src/cnn.jl
  line 3      TODO   load both sequences, french & neandertal, and tag for training
==================================================
