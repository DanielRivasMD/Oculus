- if the ancient dna motifs are observed at the end of the sequences, then cutting them short would render them modern?
- likewise, a smaller kernel migth impact the prediction power significantly?
- given that the architecture deepness is linked to the sequence length, consider using different lengths in training, or training different models
- potentially introduce regularization penalities
- add validation set
- models are mediocre, test without batchnorm
- the model length is a big concern, think about alternatives to mitigate


- techical:
  - add verbose option on cnn training
  - add logging on cnn traing

==================================================
src/cnn.jl
  line 3      TODO   load both sequences, french & neandertal, and tag for training
==================================================
